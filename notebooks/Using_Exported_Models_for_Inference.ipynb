{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Exported Models for Inference (LR & DT)\n",
    "This notebook demonstrates how to run inference using (1) exported logistic regression coefficients and (2) exported decision tree rules. It also shows how to validate those predictions against the original sklearn models (if available) and how to compute and export evaluation artifacts."
   ],
   "id": "0c30bc51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup: Imports and Paths\n",
    "We'll import the helper functions from `python_scripts/training_utils.py`, configure logging, and set up paths and a threshold used for classification."
   ],
   "id": "edc25616"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Paths\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(\"notebooks\"), '..'))\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from python_scripts.training_utils import (\n",
    "    project_paths, init_logging, load_model, load_feature_list,\n",
    "    load_lr_coefficients_csv, predict_lr_with_raw_params, predict_with_lr_pipeline,\n",
    "    load_dt_rules_json, predict_with_dt_rules, predict_with_dt_model,\n",
    "    mark_dt_threshold_predictions, render_decision_rules_text,\n",
    "    compute_performance_metrics, export_confusion_matrix_csv, compare_predictions,\n",
    " )\n",
    "\n",
    "# Initialize logging\n",
    "init_logging()\n",
    "paths = project_paths()\n",
    "\n",
    "# Load pipeline configuration\n",
    "with open(paths[\"config\"]) as f:\n",
    "    config = json.load(f)\n",
    "target_col = config[\"TARGET_COLUMN\"]  # \"isPOS\"\n",
    "\n",
    "# --- Configure file locations ---\n",
    "# Dataset: use testing if present, else fall back to training\n",
    "data_csv = paths[\"datasets\"] / \"testing_data.csv\"\n",
    "if not data_csv.exists():\n",
    "    data_csv = paths[\"datasets\"] / \"training_data.csv\"\n",
    "\n",
    "# Feature list (txt) used for both LR and DT models\n",
    "# Naming convention: {model}_{feature_space}_{weight}_{fsm}.txt\n",
    "feature_space = \"CBC_DIFF\"\n",
    "weight = \"1\"        # weight as used in model/feature filenames\n",
    "weight_f = \"1.0\"    # weight as used in export filenames (float format)\n",
    "fsm = \"all\"         # feature selection method (boruta, rfe, all)\n",
    "features_txt_lr = paths[\"features\"] / f\"lr_{feature_space}_{weight}_{fsm}.txt\"\n",
    "features_txt_dt = paths[\"features\"] / f\"dt_{feature_space}_{weight}_{fsm}.txt\"\n",
    "\n",
    "# Exported artifacts (use weight_f for export naming convention)\n",
    "lr_coeffs_csv = paths[\"root\"] / \"exports\" / f\"lr_coeffs_{feature_space}_{weight_f}_{fsm}.csv\"\n",
    "lr_pipeline_pkl = paths[\"models\"] / f\"lr_{feature_space}_{weight}_{fsm}.sav\"\n",
    "dt_rules_json = paths[\"root\"] / \"exports\" / f\"dt_rules_{feature_space}_{weight_f}_{fsm}.json\"\n",
    "dt_model_pkl = paths[\"models\"] / f\"dt_{feature_space}_{weight}_{fsm}.sav\"\n",
    "\n",
    "# Outputs\n",
    "out_dir = paths[\"root\"] / \"notebook_outputs\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Classification threshold (applies to both LR and DT)\n",
    "threshold = 0.3\n",
    "print(\"Using threshold:\", threshold)\n",
    "print(\"Target column:\", target_col)\n",
    "print(\"Feature space:\", feature_space, \"| FS method:\", fsm)"
   ],
   "id": "bb44369b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load Inference Dataset\n",
    "We'll load the dataset and align it to the expected feature set. If the labels column exists, we'll keep it for evaluation."
   ],
   "id": "6a3cfae4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (skip comment lines starting with #)\n",
    "def read_csv_skip_comments(path):\n",
    "    with open(path) as f:\n",
    "        lines = [l for l in f if not l.strip().startswith('#')]\n",
    "    return pd.read_csv(io.StringIO(''.join(lines)))\n",
    "\n",
    "df = read_csv_skip_comments(data_csv)\n",
    "print(\"Loaded:\", data_csv, \"with shape\", df.shape)\n",
    "\n",
    "# Load feature orders\n",
    "feat_order_lr = load_feature_list(features_txt_lr) if features_txt_lr.exists() else None\n",
    "feat_order_dt = load_feature_list(features_txt_dt) if features_txt_dt.exists() else None\n",
    "print(\"LR features txt:\", features_txt_lr.exists(), \"DT features txt:\", features_txt_dt.exists())\n",
    "\n",
    "# Determine target label if present\n",
    "y = df[target_col].astype(int) if target_col in df.columns else None\n",
    "if y is not None:\n",
    "    print(f\"Labels found: {(y == 0).sum()} neg / {(y == 1).sum()} pos\")\n",
    "\n",
    "# Align and coerce feature matrices\n",
    "def align_X(df_in: pd.DataFrame, feat_order: list[str]) -> pd.DataFrame:\n",
    "    missing = [c for c in feat_order if c not in df_in.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns for inference: {missing}\")\n",
    "    X = df_in[feat_order].copy().astype(float)\n",
    "    # Drop rows with missing values in required columns\n",
    "    X = X.dropna(subset=feat_order)\n",
    "    return X\n",
    "\n",
    "X_lr = align_X(df, feat_order_lr) if feat_order_lr else None\n",
    "X_dt = align_X(df, feat_order_dt) if feat_order_dt else None\n",
    "print(\"X_lr shape:\", None if X_lr is None else X_lr.shape, \"| X_dt shape:\", None if X_dt is None else X_dt.shape)"
   ],
   "id": "ebf58aec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf58aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(data_csv)\n",
    "print(\"Loaded:\", data_csv, \"with shape\", df.shape)\n",
    "\n",
    "# Load feature orders\n",
    "feat_order_lr = load_feature_list(features_txt_lr) if features_txt_lr.exists() else None\n",
    "feat_order_dt = load_feature_list(features_txt_dt) if features_txt_dt.exists() else None\n",
    "print(\"LR features txt:\", features_txt_lr.exists(), \"DT features txt:\", features_txt_dt.exists())\n",
    "\n",
    "# Determine target label if present\n",
    "target_col = \"isPOS\" if \"isPOS\" in df.columns else None\n",
    "y = df[target_col].astype(int) if target_col else None\n",
    "\n",
    "# Align and coerce feature matrices\n",
    "def align_X(df_in: pd.DataFrame, feat_order: list[str]) -> pd.DataFrame:\n",
    "    missing = [c for c in feat_order if c not in df_in.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns for inference: {missing}\")\n",
    "    X = df_in[feat_order].copy().astype(float)\n",
    "    # Drop rows with missing values in required columns\n",
    "    X = X.dropna(subset=feat_order)\n",
    "    return X\n",
    "\n",
    "X_lr = align_X(df, feat_order_lr) if feat_order_lr else None\n",
    "X_dt = align_X(df, feat_order_dt) if feat_order_dt else None\n",
    "print(\"X_lr shape:\", None if X_lr is None else X_lr.shape, \"| X_dt shape:\", None if X_dt is None else X_dt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023bf700",
   "metadata": {},
   "source": [
    "## 3) Load Logistic Regression Coefficients\n",
    "Load the raw-space coefficients and intercept exported from the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LR coefficients CSV\n",
    "if not lr_coeffs_csv.exists():\n",
    "    raise FileNotFoundError(f\"Missing LR coefficients CSV: {lr_coeffs_csv}\")\n",
    "feat_order_lr_csv, weights_raw, intercept_raw = load_lr_coefficients_csv(lr_coeffs_csv)\n",
    "print(\"Loaded LR coeffs for\", len(feat_order_lr_csv), \"features\")\n",
    "\n",
    "# Ensure X_lr aligns with the order in the coefficients file\n",
    "if X_lr is None:\n",
    "    X_lr = align_X(df, feat_order_lr_csv)\n",
    "else:\n",
    "    # Reorder to match coefficients if necessary\n",
    "    X_lr = X_lr[feat_order_lr_csv].copy()\n",
    "X_lr = X_lr.astype(float)\n",
    "\n",
    "# Note: scoring uses z = Xw + b and p = 1/(1+exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5448da8",
   "metadata": {},
   "source": [
    "## 4) Predict with Logistic Coefficients (Raw Space)\n",
    "Run logistic regression inference using the exported coefficients. If labels are available, compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with LR coefficients\n",
    "proba_lr, preds_lr = predict_lr_with_raw_params(\n",
    "    X_lr, feat_order_lr_csv, weights_raw, intercept_raw, threshold=threshold\n",
    " )\n",
    "print(\"LR predictions:\", len(preds_lr))\n",
    "\n",
    "# Show head\n",
    "display(pd.DataFrame({\n",
    "    \"proba_lr\": proba_lr[:10],\n",
    "    \"preds_lr\": preds_lr[:10],\n",
    "}))\n",
    "\n",
    "# Metrics if labels available\n",
    "if y is not None:\n",
    "    # Ensure y is aligned to X rows (drop rows that were removed due to NA)\n",
    "    y_aligned = y.loc[X_lr.index].values.astype(int)\n",
    "    metrics_lr = compute_performance_metrics(y_aligned, preds_lr, proba_lr)\n",
    "    print(\"LR metrics:\", json.dumps(metrics_lr, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5875d4",
   "metadata": {},
   "source": [
    "## 5) Validate LR Coefficient Inference vs Pipeline\n",
    "If the original LR pipeline is available, we can compare the coefficient-based predictions to the pipeline outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate vs pipeline (if available)\n",
    "if lr_pipeline_pkl.exists():\n",
    "    pipeline = load_model(lr_pipeline_pkl)\n",
    "    proba_skl, preds_skl = predict_with_lr_pipeline(pipeline, X_lr, threshold=threshold)\n",
    "    cmp = compare_predictions(proba_lr, preds_lr, proba_skl, preds_skl, prob_tol=1e-9)\n",
    "    print(\"LR compare (coeffs vs pipeline):\", json.dumps(cmp, indent=2))\n",
    "    if y is not None:\n",
    "        y_aligned = y.loc[X_lr.index].values.astype(int)\n",
    "        print(\"LR pipeline metrics:\")\n",
    "        print(json.dumps(compute_performance_metrics(y_aligned, preds_skl, proba_skl), indent=2))\n",
    "else:\n",
    "    print(\"LR pipeline pickle not found:\", lr_pipeline_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510a0d5",
   "metadata": {},
   "source": [
    "## 6) Load Decision Tree Rules\n",
    "Load the exported JSON rules and (optionally) annotate leaves with threshold-based predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DT rules JSON\n",
    "if not dt_rules_json.exists():\n",
    "    raise FileNotFoundError(f\"Missing DT rules JSON: {dt_rules_json}\")\n",
    "tree_dict, saved_thr = load_dt_rules_json(dt_rules_json)\n",
    "print(\"Loaded DT rules. Saved threshold in file:\", saved_thr)\n",
    "\n",
    "# Optionally mark leaves using current threshold\n",
    "tree_dict = mark_dt_threshold_predictions(tree_dict, threshold)\n",
    "\n",
    "# Feature order for DT is described by the features txt (as exported at training time)\n",
    "if X_dt is None:\n",
    "    # Fall back to LR feature order if DT features file is missing but rules reference the same names\n",
    "    # Otherwise, ensure you create a DT features file matching your model artifacts.\n",
    "    if feat_order_lr is None:\n",
    "        raise FileNotFoundError(\"DT features list not found. Ensure a matching features txt exists.\")\n",
    "    X_dt = align_X(df, feat_order_lr)\n",
    "feature_order_dt = list(X_dt.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee90a3",
   "metadata": {},
   "source": [
    "## 7) Predict with Decision Tree Rules\n",
    "Infer probabilities and labels from the rules; thresholding converts probabilities to binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the rules\n",
    "proba_dt_rules, preds_dt_leaf = predict_with_dt_rules(X_dt, list(X_dt.columns), tree_dict)\n",
    "preds_dt_thresh = (proba_dt_rules >= threshold).astype(int)\n",
    "print(\"DT rule-based predictions:\", len(preds_dt_thresh))\n",
    "\n",
    "# Show head\n",
    "display(pd.DataFrame({\n",
    "    \"proba_dt_rules\": proba_dt_rules[:10],\n",
    "    \"preds_dt_thresh\": preds_dt_thresh[:10],\n",
    "    \"preds_dt_leaf\": preds_dt_leaf[:10],\n",
    "}))\n",
    "\n",
    "# Metrics if labels available\n",
    "if y is not None:\n",
    "    y_aligned_dt = y.loc[X_dt.index].values.astype(int)\n",
    "    metrics_dt = compute_performance_metrics(y_aligned_dt, preds_dt_thresh, proba_dt_rules)\n",
    "    print(\"DT rule metrics:\", json.dumps(metrics_dt, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75e325",
   "metadata": {},
   "source": [
    "## 8) Validate Decision Tree Rule Inference vs Model\n",
    "If the sklearn DT model is available, compare rule-based predictions to the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate vs DT model (if available)\n",
    "if dt_model_pkl.exists():\n",
    "    dt_model = load_model(dt_model_pkl)\n",
    "    proba_dt_skl, preds_dt_skl = predict_with_dt_model(dt_model, X_dt, threshold=threshold)\n",
    "    cmp_dt = compare_predictions(proba_dt_rules, preds_dt_thresh, proba_dt_skl, preds_dt_skl, prob_tol=1e-9)\n",
    "    print(\"DT compare (rules vs model):\", json.dumps(cmp_dt, indent=2))\n",
    "    if y is not None:\n",
    "        y_aligned_dt = y.loc[X_dt.index].values.astype(int)\n",
    "        print(\"DT sklearn metrics:\")\n",
    "        print(json.dumps(compute_performance_metrics(y_aligned_dt, preds_dt_skl, proba_dt_skl), indent=2))\n",
    "else:\n",
    "    print(\"DT model pickle not found:\", dt_model_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28323fd5",
   "metadata": {},
   "source": [
    "## 9) Render and Inspect Decision Rules Text\n",
    "You can render human-readable rules from the JSON tree and inspect leaf annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f347f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render rules text\n",
    "lines = render_decision_rules_text(tree_dict)\n",
    "print(\"\\n\".join(lines[:200]))  # print the first ~200 lines for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea80ff1",
   "metadata": {},
   "source": [
    "## 10) Compute and Export Evaluation Artifacts\n",
    "If labels are available, we can export confusion matrices and a combined results CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export artifacts (optional)\n",
    "if y is not None:\n",
    "    y_lr = y.loc[X_lr.index].values.astype(int)\n",
    "    y_dt = y.loc[X_dt.index].values.astype(int)\n",
    "    # Confusion matrices\n",
    "    export_confusion_matrix_csv(out_dir / \"cm_lr_from_coeffs.csv\", y_lr, preds_lr.astype(int))\n",
    "    export_confusion_matrix_csv(out_dir / \"cm_dt_from_rules.csv\", y_dt, preds_dt_thresh.astype(int))\n",
    "    print(\"Exported confusion matrices to:\", out_dir)\n",
    "\n",
    "    # Combined results CSV\n",
    "    df_out = pd.DataFrame({\n",
    "        \"index\": X_lr.index,\n",
    "        \"proba_lr\": proba_lr,\n",
    "        \"preds_lr\": preds_lr,\n",
    "        \"proba_dt_rules\": proba_dt_rules,\n",
    "        \"preds_dt_thresh\": preds_dt_thresh,\n",
    "    })\n",
    "    if target_col:\n",
    "        df_out[target_col] = y.loc[X_lr.index].values.astype(int)\n",
    "    df_out.to_csv(out_dir / \"inference_results_lr_dt.csv\", index=False)\n",
    "    print(\"Wrote:\", out_dir / \"inference_results_lr_dt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab426f8",
   "metadata": {},
   "source": [
    "## 11) Optional: Batch Scoring Functions (LR and DT)\n",
    "Reusable helpers for batch scoring from CSV input files. Update paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "def score_with_lr_coeffs(input_csv: Path, features_txt: Path, coeffs_csv: Path, out_csv: Path, threshold: float = 0.3, target_col: Optional[str] = \"isPOS\") -> Tuple[pd.DataFrame, Optional[dict]]:\n",
    "    df_in = pd.read_csv(input_csv)\n",
    "    feat_order = load_feature_list(features_txt)\n",
    "    X = df_in[feat_order].copy().astype(float).dropna(subset=feat_order)\n",
    "    y_local = df_in[target_col].astype(int).loc[X.index].values if (target_col and target_col in df_in.columns) else None\n",
    "    feats, weights, intercept = load_lr_coefficients_csv(coeffs_csv)\n",
    "    proba, preds = predict_lr_with_raw_params(X[feats], feats, weights, intercept, threshold=threshold)\n",
    "    out_df = pd.DataFrame({\"index\": X.index, \"proba_lr\": proba, \"preds_lr\": preds})\n",
    "    out_df.to_csv(out_csv, index=False)\n",
    "    metrics = compute_performance_metrics(y_local, preds, proba) if y_local is not None else None\n",
    "    return out_df, metrics\n",
    "\n",
    "def score_with_dt_rules(input_csv: Path, features_txt: Path, rules_json: Path, out_csv: Path, threshold: float = 0.3, target_col: Optional[str] = \"isPOS\") -> Tuple[pd.DataFrame, Optional[dict]]:\n",
    "    df_in = pd.read_csv(input_csv)\n",
    "    feat_order = load_feature_list(features_txt)\n",
    "    X = df_in[feat_order].copy().astype(float).dropna(subset=feat_order)\n",
    "    y_local = df_in[target_col].astype(int).loc[X.index].values if (target_col and target_col in df_in.columns) else None\n",
    "    tree, saved_thr = load_dt_rules_json(rules_json)\n",
    "    tree = mark_dt_threshold_predictions(tree, threshold)\n",
    "    proba, preds_leaf = predict_with_dt_rules(X, feat_order, tree)\n",
    "    preds = (proba >= threshold).astype(int)\n",
    "    out_df = pd.DataFrame({\"index\": X.index, \"proba_dt_rules\": proba, \"preds_dt_thresh\": preds, \"preds_dt_leaf\": preds_leaf})\n",
    "    out_df.to_csv(out_csv, index=False)\n",
    "    metrics = compute_performance_metrics(y_local, preds, proba) if y_local is not None else None\n",
    "    return out_df, metrics\n",
    "\n",
    "print(\"Defined batch scoring helpers: score_with_lr_coeffs, score_with_dt_rules\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blood-culture-outcome-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}