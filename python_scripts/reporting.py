"""
Reporting module for generating HTML summaries of training and inference runs.
"""

from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd


CSS_STYLES = """
<style>
    body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; margin: 2rem; color: #333; line-height: 1.6; }
    h1, h2, h3 { color: #2c3e50; margin-top: 2rem; }
    h1 { border-bottom: 2px solid #eee; padding-bottom: 0.5rem; }
    .meta-box { background: #f8f9fa; padding: 1rem; border-radius: 4px; border: 1px solid #e9ecef; margin-bottom: 2rem; }
    .meta-box p { margin: 0.25rem 0; }
    table { border-collapse: collapse; width: 100%; margin-bottom: 1rem; font-size: 0.9rem; }
    th, td { padding: 0.75rem; text-align: left; border-bottom: 1px solid #ddd; }
    th { background-color: #f1f3f5; font-weight: 600; }
    tr:hover { background-color: #f8f9fa; }
    .status-badge { display: inline-block; padding: 0.25em 0.6em; font-size: 75%; font-weight: 700; line-height: 1; color: #fff; text-align: center; white-space: nowrap; vertical-align: baseline; border-radius: 0.25rem; }
    .bg-success { background-color: #28a745; }
    .bg-warning { background-color: #ffc107; color: #212529; }
    .bg-danger { background-color: #dc3545; }
    .footer { margin-top: 4rem; font-size: 0.85rem; color: #6c757d; border-top: 1px solid #eee; padding-top: 1rem; }
</style>
"""


def _render_html(title: str, content: str) -> str:
    """Wrap content in a basic HTML5 boilerplate with CSS."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    {CSS_STYLES}
</head>
<body>
    <h1>{title}</h1>
    <div class="meta-box">
        <p><strong>Generated:</strong> {timestamp}</p>
    </div>
    {content}
    <div class="footer">
        Generated by blood-culture-outcome-classification pipeline.
    </div>
</body>
</html>
"""


def generate_training_report(
    output_path: Path,
    dataset_info: Dict[str, Any],
    results_df: pd.DataFrame,
    config: Dict[str, Any],
    plots: Dict[str, Dict[str, str]] = None,
) -> None:
    """Generate an HTML report for the training run.
    
    plots: Dict mapping model_name -> {'roc': 'base64str', 'cm': 'base64str'}
    """
    
    # Section 1: Dataset & Config
    stats_html = f"""
    <h2>Run Configuration & Statistics</h2>
    <div class="meta-box">
        <p><strong>Total Samples:</strong> {dataset_info.get('n_total', 'N/A')}</p>
        <p><strong>Positive Class (1):</strong> {dataset_info.get('n_pos', 'N/A')} ({dataset_info.get('pct_pos', 0):.1f}%)</p>
        <p><strong>Negative Class (0):</strong> {dataset_info.get('n_neg', 'N/A')} ({dataset_info.get('pct_neg', 0):.1f}%)</p>
        <p><strong>Random State:</strong> {config.get('RANDOM_STATE', 'N/A')}</p>
    </div>
    """

    # Section 2: Results Table (Enhanced with Plots)
    if not results_df.empty and "balanced_accuracy_mean" in results_df.columns:
        results_df = results_df.sort_values("balanced_accuracy_mean", ascending=False)

    table_html = """
    <h2>Cross-Validation Results (Nested CV)</h2>
    <p>Ranked by Balanced Accuracy (descending). Performance metrics are now unbiased (feature selection performed inside CV loop).</p>
    """
    
    if not results_df.empty:
        # Standard table
        display_cols = [
            "model",
            "balanced_accuracy_mean", "balanced_accuracy_std",
            "recall_mean", "recall_std",
            "specificity_mean", "specificity_std",
            "roc_auc_mean", "precision_mean"
        ]
        display_cols = [c for c in display_cols if c in results_df.columns]
        numeric_cols = results_df.select_dtypes(include=['float', 'int']).columns
        df_display = results_df.copy()
        df_display[numeric_cols] = df_display[numeric_cols].round(3)
        table_html += df_display[display_cols].to_html(index=False, classes="table", border=0)
        
        # Section 3: Performance Visualization
        if plots:
            table_html += "<h2>Performance Visualization</h2>"
            for _, row in results_df.iterrows():
                m_name = row["model"]
                m_plots = plots.get(m_name, {})
                rc = m_plots.get("roc")
                cm = m_plots.get("cm")
                
                if rc or cm:
                    table_html += f"<h3>Model: {m_name}</h3><div style='display: flex; flex-wrap: wrap; gap: 20px; margin-bottom: 2rem;'>"
                    if rc:
                        table_html += f"<div><img src='data:image/png;base64,{rc}' alt='ROC Curve {m_name}' style='max-width: 400px; border: 1px solid #ddd; padding: 5px;'></div>"
                    if cm:
                        table_html += f"<div><img src='data:image/png;base64,{cm}' alt='Confusion Matrix {m_name}' style='max-width: 400px; border: 1px solid #ddd; padding: 5px;'></div>"
                    table_html += "</div><hr>"

    else:
        table_html += "<p>No results found.</p>"

    full_html = _render_html("Training Report", stats_html + table_html)
    output_path.write_text(full_html)


def generate_inference_report(
    output_path: Path,
    input_info: Dict[str, Any],
    predictions_summary: List[Dict[str, Any]],
    validation_results: Optional[Dict[str, Any]] = None,
    plots: Dict[str, Dict[str, str]] = None,
    metrics: Optional[List[Dict[str, Any]]] = None,
) -> None:
    """Generate an HTML report for the inference run.
    
    plots: Dict mapping model_name -> {'roc': 'base64str', 'cm': 'base64str'}
    metrics: List of dicts from compute_performance_metrics (one per model)
    """
    
    # Section 1: Input Summary
    stats_html = f"""
    <h2>Input Dataset Summary</h2>
    <div class="meta-box">
        <p><strong>Input File:</strong> {input_info.get('filename', 'N/A')}</p>
        <p><strong>Total Rows:</strong> {input_info.get('n_total', 0)}</p>
        <p><strong>Processed Rows:</strong> {input_info.get('n_processed', 0)}</p>
    </div>
    """

    # Section 2: Performance Metrics (if ground truth was available)
    metrics_html = ""
    if metrics:
        metrics_html = "<h2>Performance Metrics</h2>"
        metrics_df = pd.DataFrame(metrics)
        display_cols = [
            "model", "balanced_accuracy", "recall", "specificity",
            "roc_auc", "precision",
        ]
        display_cols = [c for c in display_cols if c in metrics_df.columns]
        numeric_cols = metrics_df[display_cols].select_dtypes(include=["float", "int"]).columns
        df_display = metrics_df.copy()
        df_display[numeric_cols] = df_display[numeric_cols].round(3)
        metrics_html += df_display[display_cols].to_html(index=False, classes="table", border=0)

    # Section 3: Model Predictions
    models_html = "<h2>Model Predictions</h2>"
    
    if predictions_summary:
        # Create a summary DataFrame
        summary_df = pd.DataFrame(predictions_summary)
        models_html += summary_df.to_html(index=False, classes="table", border=0)
        
        # Section 4: Visualization (if provided)
        if plots:
            models_html += "<h2>Visualizations</h2>"
            for row in predictions_summary:
                m_name = row["model"]
                m_plots = plots.get(m_name, {})
                rc = m_plots.get("roc")
                cm = m_plots.get("cm")
                
                if rc or cm:
                    models_html += f"<h3>Model: {m_name}</h3><div style='display: flex; flex-wrap: wrap; gap: 20px; margin-bottom: 2rem;'>"
                    if rc:
                        models_html += f"<div><img src='data:image/png;base64,{rc}' alt='ROC Curve {m_name}' style='max-width: 400px; border: 1px solid #ddd; padding: 5px;'></div>"
                    if cm:
                        models_html += f"<div><img src='data:image/png;base64,{cm}' alt='Confusion Matrix {m_name}' style='max-width: 400px; border: 1px solid #ddd; padding: 5px;'></div>"
                    models_html += "</div><hr>"

    else:
        models_html += "<p>No predictions generated.</p>"

    # Section 5: Validation (if applicable)
    val_html = ""
    if validation_results:
        val_html = "<h2>Internal Validation</h2>"
        # Convert nested dict to a flat table or list
        val_items = []
        for model_name, res in validation_results.items():
            status = "PASS" if res.get("preds_equal") else "FAIL"
            badge_class = "bg-success" if status == "PASS" else "bg-danger"
            val_items.append(
                f"<li><strong>{model_name}:</strong> <span class='status-badge {badge_class}'>{status}</span> "
                f"(Diffs: {res.get('mismatch_count', 0)}, Max prob diff: {res.get('prob_max_abs_diff', 0):.2e})</li>"
            )
        val_html += f"<ul>{''.join(val_items)}</ul>"

    full_html = _render_html("Inference Report", stats_html + metrics_html + models_html + val_html)
    output_path.write_text(full_html)
