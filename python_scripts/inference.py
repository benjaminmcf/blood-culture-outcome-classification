"""
Inference script for blood culture outcome classification models.

Rules:
- Logistic Regression: extract raw-space coefficients from the pipeline and use them
  directly (without the model object) at threshold=0.3. Export coefficients.
- Decision Tree: extract human-readable rules and use those for inference (no model object).
  Export rules in text and JSON.
- Other models (RF, XG): use the model objects for inference.

The features used for inference are loaded from the corresponding features/*.txt files.
Outputs:
- predictions/*.csv for each model (prob and label)
- exports/lr_coeffs_*.csv for LR
- exports/dt_rules_*.txt and dt_rules_*.json for DT
"""

from __future__ import annotations

import io
import json
import logging
import argparse
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd

from .training_utils import (
    init_logging,
    project_paths,
    load_config,
    load_model,
    load_feature_list,
    load_lr_coefficients_csv,
    load_dt_rules_json,
    compute_performance_metrics,
    extract_lr_raw_params_from_pipeline,
    predict_lr_with_raw_params,
    export_lr_coefficients_csv,
    extract_decision_tree_rules,
    render_decision_rules_text,
    predict_with_dt_rules,
    export_confusion_matrix_csv,
    predict_with_lr_pipeline,
    predict_with_dt_model,
    compare_predictions,
    plot_roc_curve,
    plot_confusion_matrix,
)
from .reporting import generate_inference_report

DEFAULT_THRESHOLD = 0.3


def load_data_filtering_comments(path: Path) -> pd.DataFrame:
    """Load CSV filtering lines starting with '#' but preserving '#' in content."""
    with path.open("r") as f:
        lines = [line for line in f if not line.strip().startswith("#")]
    return pd.read_csv(io.StringIO("".join(lines)))



def list_model_artifacts(models_dir: Path, features_dir: Path) -> List[dict]:
    """Find model configurations by looking for metadata JSON files.

    Returns list of metadata dicts, each containing path info.
    """
    configs: List[dict] = []
    # Look for .json files in models dir (generated by training with save_model_metadata)
    for meta_path in models_dir.glob("*.json"):
        try:
            meta = json.loads(meta_path.read_text())
            # Ensure paths exist
            model_path = models_dir / meta["model_path"]
            # Features are in the metadata, but we can also check the file exists if needed
            # feature_list_path = features_dir / meta["feature_list_path"]
            
            if model_path.exists():
                meta["full_model_path"] = model_path
                # meta["full_feature_path"] = feature_list_path
                configs.append(meta)
        except Exception as e:
            logging.warning(f"Failed to load metadata from {meta_path}: {e}")
            continue

    # Fallback for old-style filenames if no json found? 
    # For now, we assume the new training script was run.
    if not configs:
         logging.warning("No metadata JSON files found in %s. Re-run training to generate them.", models_dir)
         
    return configs


essential_newline = "\n"

def ensure_dirs(paths: List[Path]):
    for p in paths:
        p.mkdir(parents=True, exist_ok=True)


def run_inference(
    df: pd.DataFrame,
    *,
    threshold: float = DEFAULT_THRESHOLD,
    validate: bool = False,
    input_name: str = "unknown",
) -> pd.DataFrame:
    from pathlib import Path as _Path
    _repo_root = _Path(__file__).resolve().parent.parent
    cfg = load_config(_repo_root / "config.json")
    paths = project_paths(cfg)
    models_dir = paths["models"]
    features_dir = paths["features"]
    exports_dir = paths["root"] / "exports"
    predictions_dir = paths["root"] / "predictions"
    ensure_dirs([exports_dir, predictions_dir])

    outputs: List[pd.DataFrame] = []
    metrics_rows: List[dict] = []
    plots_dict = {}

    target_col = cfg["TARGET_COLUMN"]

    for meta in list_model_artifacts(models_dir, features_dir):
        model_key = meta["model_key"]
        feature_space = meta["feature_space"]
        weight = meta["weight"]
        fsm = meta["fsm"]
        features = meta["features"]
        model_path = meta["full_model_path"]
        
        # Use weight formatted to 2 decimals for consistency in naming if needed, or just use what's in meta
        # The key for output filenames:
        model_name = f"{model_key}_{feature_space}_{weight}_{fsm}"
        
        logging.info("Running inference for %s", model_name)
        
        # No need to load features from txt, we have them in metadata
        features_and_target = features + [target_col]
        df_temp = df.copy()

        if target_col in df.columns:
            # Check if all features exist in df; if not, warn and skip? 
            missing_cols = [c for c in features_and_target if c not in df_temp.columns]
            if missing_cols:
                logging.warning("Missing columns for model %s: %s. Skipping.", model_name, missing_cols)
                continue
                
            df_temp = df_temp[features_and_target]
            # drop rows with missing values in any of the features or target
            df_temp = df_temp.dropna(subset=features_and_target)
            y_true = df_temp[target_col].astype(int)
            df_temp = df_temp.drop(columns=[target_col])  # ensure we only use features
        else:
             missing_cols = [c for c in features if c not in df_temp.columns]
             if missing_cols:
                logging.warning("Missing columns for model %s: %s. Skipping.", model_name, missing_cols)
                continue
             df_temp = df_temp[features]
             df_temp = df_temp.dropna(subset=features)
             y_true = None

        X = df_temp[features].copy()

        if model_key == "lr":
            # Prefer existing exported coefficients; fallback to pipeline extraction
            coeffs_csv = exports_dir / f"lr_coeffs_{feature_space}_{weight}_{fsm}.csv"
            if coeffs_csv.exists():
                feats_order, weights_raw, intercept_raw = load_lr_coefficients_csv(coeffs_csv)
                # Verify features match? 
                if feats_order != features:
                     logging.warning("Exported coefficients features mismatch for %s. Re-extracting.", model_name)
                     # fall through to extraction
                     pipeline = load_model(model_path)
                     params = extract_lr_raw_params_from_pipeline(pipeline, features)
                     probs, preds = predict_lr_with_raw_params(
                         X,
                         feature_order=params["features"],
                         weights_raw=params["weights_raw"],
                         intercept_raw=params["intercept_raw"],
                         threshold=threshold,
                     )
                     export_lr_coefficients_csv(
                         coeffs_csv, params["features"], params["weights_raw"], params["intercept_raw"]
                     )
                     pipeline = None # Reset
                else: 
                     probs, preds = predict_lr_with_raw_params(
                        X,
                        feature_order=feats_order,
                        weights_raw=weights_raw,
                        intercept_raw=intercept_raw,
                        threshold=threshold,
                    )
                     pipeline = None
            else:
                pipeline = load_model(model_path)
                params = extract_lr_raw_params_from_pipeline(pipeline, features)
                probs, preds = predict_lr_with_raw_params(
                    X,
                    feature_order=params["features"],
                    weights_raw=params["weights_raw"],
                    intercept_raw=params["intercept_raw"],
                    threshold=threshold,
                )
                # Export coefficients for future runs
                export_lr_coefficients_csv(
                    coeffs_csv, params["features"], params["weights_raw"], params["intercept_raw"]
                )

            # Optional validation: if a pipeline was loaded, compare against it
            if validate and 'pipeline' in locals() and pipeline is not None:
                proba_b, preds_b = predict_with_lr_pipeline(pipeline, X, threshold=threshold)
                report = compare_predictions(probs, preds, proba_b, preds_b)
                (exports_dir / "validation").mkdir(parents=True, exist_ok=True)
                (exports_dir / "validation" / f"validate_lr_{feature_space}_{weight}_{fsm}.json").write_text(
                    json.dumps(report, indent=2)
                )

        elif model_key == "dt":
            # Prefer existing exported rules; fallback to model extraction
            rules_txt = exports_dir / f"dt_rules_{feature_space}_{weight}_{fsm}.txt"
            rules_json = exports_dir / f"dt_rules_{feature_space}_{weight}_{fsm}.json"
            if rules_json.exists():
                tree_dict, saved_thr = load_dt_rules_json(rules_json)
                # Re-annotate with current threshold if different
                from .training_utils import mark_dt_threshold_predictions
                tree_dict = mark_dt_threshold_predictions(tree_dict, threshold)
                dt_model = None
            else:
                dt_model = load_model(model_path)
                tree_dict = extract_decision_tree_rules(dt_model, features)
                # Annotate leaves with threshold-based predictions
                from .training_utils import mark_dt_threshold_predictions
                tree_dict = mark_dt_threshold_predictions(tree_dict, threshold)
                # Save text (prepend threshold info) and JSON
                lines = render_decision_rules_text(tree_dict)
                header = [f"# probability_threshold={threshold:.6f}"]
                rules_txt.write_text(essential_newline.join(header + lines) + essential_newline)
                export_obj = {"probability_threshold": float(threshold), "tree": tree_dict}
                rules_json.write_text(json.dumps(export_obj, indent=2))

            probs, preds_leaf = predict_with_dt_rules(X, features, tree_dict)
            # Apply probability threshold to DT probabilities
            preds = (probs >= threshold).astype(int)

            # Optional validation: compare against DT model predictions if model loaded
            if validate and 'dt_model' in locals() and dt_model is not None:
                proba_b, preds_b = predict_with_dt_model(dt_model, X, threshold=threshold)
                report = compare_predictions(probs, preds, proba_b, preds_b)
                (exports_dir / "validation").mkdir(parents=True, exist_ok=True)
                (exports_dir / "validation" / f"validate_dt_{feature_space}_{weight}_{fsm}.json").write_text(
                    json.dumps(report, indent=2)
                )

        else:
            # RF, XG: use model directly
            model = load_model(model_path)
            try:
                probs = model.predict_proba(X.values)[:, 1]
            except Exception:
                # fallback to decision function or binary prediction
                if hasattr(model, "decision_function"):
                    margins = model.decision_function(X.values)
                    probs = 1.0 / (1.0 + np.exp(-margins))
                else:
                    preds = model.predict(X.values)
                    probs = preds.astype(float)
            preds = (probs >= threshold).astype(int)

        # Save predictions
        out_payload = {
            "model": model_name,
            "prob_pos": probs,
            "yhat": preds,
        }
        # Include the leaf-based DT predictions for transparency
        if model_key == "dt":
            out_payload["yhat_leaf"] = preds_leaf
        out_df = pd.DataFrame(out_payload)
        out_path = predictions_dir / f"preds_{model_name}.csv"
        out_df.to_csv(out_path, index=False)
        outputs.append(out_df)

        # If ground truth provided, export confusion matrix and collect metrics
        if y_true is not None and len(y_true) == len(out_df):
            cm_path = predictions_dir / f"cm_{model_name}.csv"
            export_confusion_matrix_csv(cm_path, y_true.values.astype(int), preds.astype(int))
            # Metrics summary per model
            perf = compute_performance_metrics(y_true.values.astype(int), preds.astype(int), probs)
            perf_row = {"model": model_name, "threshold": float(threshold)}
            perf_row.update(perf)
            metrics_rows.append(perf_row)
            
            # Generate Plots for Report
            roc_b64 = plot_roc_curve(y_true.values.astype(int), probs, title=f"ROC: {model_name}")
            cm_b64 = plot_confusion_matrix(y_true.values.astype(int), preds.astype(int), title=f"Confusion Matrix: {model_name}")
            plots_dict[model_name] = {"roc": roc_b64, "cm": cm_b64}
            
    # Write aggregated metrics if available
    validation_results = {}
    if metrics_rows:
        metrics_df = pd.DataFrame(metrics_rows)
        metrics_out = predictions_dir / "metrics_summary.csv"
        metrics_df.to_csv(metrics_out, index=False)
        logging.info("Wrote metrics summary to %s", metrics_out)

    # Validate reports logic: if validation was run, we might have JSONs in exports/validation
    # Re-reading them for the report is cleaner
    if validate:
        val_dir = exports_dir / "validation"
        if val_dir.exists():
            for js_path in val_dir.glob("*.json"):
                 try:
                    res = json.loads(js_path.read_text())
                    # Extract concise model name from filename validate_MODEL.json
                    m_name = js_path.stem.replace("validate_", "")
                    validation_results[m_name] = res
                 except Exception:
                     pass

    # Generate HTML Report
    # We need simple summary stats for the report
    unique_models = sorted(list({o["model"][0] for o in outputs})) if outputs else []
    
    # Collect plots from the loop? 
    # I need to restructure slightly to capture plots.
    # Let's assume plots_dict was populated in the loop.
    
    predictions_summary = []
    
    # Summarize per model output
    for out_df in outputs:
        m_name = out_df["model"].iloc[0]
        n_preds = len(out_df)
        n_pos_pred = int(out_df["yhat"].sum())
        predictions_summary.append({
            "model": m_name,
            "n_predictions": n_preds,
            "n_positive_pred": n_pos_pred,
            "pct_positive": round((n_pos_pred / n_preds * 100), 1) if n_preds else 0
        })

    input_info = {
        "filename": input_name,
        "n_total": len(df),
        "n_processed": len(outputs[0]) if outputs else 0
    }
    
    report_path = predictions_dir / "inference_report.html"
    generate_inference_report(
        output_path=report_path,
        input_info=input_info,
        predictions_summary=predictions_summary,
        validation_results=validation_results if validate else None,
        plots=plots_dict,
        metrics=metrics_rows,
    )
    logging.info("Wrote inference report to %s", report_path)

    return pd.concat(outputs, ignore_index=True) if outputs else pd.DataFrame()


def main():
    parser = argparse.ArgumentParser(description="Inference for blood culture outcome models")
    parser.add_argument("--threshold", type=float, default=DEFAULT_THRESHOLD, help="Probability threshold for binary predictions (default: 0.3)")
    parser.add_argument("--validate", action="store_true", help="Validate LR/DT extracted predictions against model objects and write a JSON report")
    parser.add_argument("--input", type=Path, help="Path to input CSV. Defaults to testing_data.csv if present, else training_data.csv")
    args = parser.parse_args()

    init_logging()
    from pathlib import Path as _Path
    _repo_root = _Path(__file__).resolve().parent.parent
    config = load_config(_repo_root / "config.json")
    paths = project_paths(config)
    
    if args.input:
        df_all = load_data_filtering_comments(args.input)
    else:
        # Default: if a testing dataset exists, use it; otherwise fall back to training.
        test_path = paths["datasets"] / "testing_data.csv"
        if test_path.exists():
            df_all = load_data_filtering_comments(test_path)
            logging.info("Using default test set: %s", test_path)
        else:
            df_all = load_data_filtering_comments(paths["datasets"] / "training_data.csv")
            logging.info("Using training data for inference: %s", paths["datasets"] / "training_data.csv")

    preds = run_inference(df_all, threshold=args.threshold, validate=args.validate, input_name=args.input.name if args.input else "default/training")
    logging.info("Inference complete. Rows: %d", len(preds))


if __name__ == "__main__":
    main()
